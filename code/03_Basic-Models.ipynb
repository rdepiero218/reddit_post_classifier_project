{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3: Web APIs & NLP\n",
    "## *Basic Models*\n",
    "\n",
    "In this notebook:\n",
    "\n",
    "* [Logistic Regression](#lgr)\n",
    "* [KNN](#knn)\n",
    "* [Basic Decision Tree](#dtree)\n",
    "* [Naive Bayes](#naive-bayes)\n",
    "\n",
    "These are all basic models with minimal parameter selections to get a baseline understanding of how different models perform.\n",
    "\n",
    "#### Import Libraries & Read in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## standard imports \n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import re\n",
    "## visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "## preprocessing\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.dummy import DummyClassifier\n",
    "## modeling\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression, Lasso, Ridge, ElasticNet\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "## trees\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.ensemble import BaggingClassifier, BaggingRegressor, RandomForestClassifier, ExtraTreesClassifier, RandomForestRegressor, ExtraTreesRegressor, AdaBoostRegressor, AdaBoostClassifier, GradientBoostingRegressor\n",
    "## NLP\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "## analysis\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix, accuracy_score, make_scorer, f1_score, mean_squared_error\n",
    "\n",
    "\n",
    "## options\n",
    "import sklearn\n",
    "pd.options.display.max_rows = 4000\n",
    "pd.options.display.max_columns = 100\n",
    "pd.set_option('max_colwidth', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### read in data\n",
    "path = '../data/reddit_posts_clean.csv'\n",
    "data = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Select Data\n",
    "X = data['selftext']\n",
    "y = data['is_fallout']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TTS\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Null Model <a class=\"anchor\" id=\"null-model\"></a>\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4754\n",
       "1    4318\n",
       "Name: is_fallout, dtype: int64"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5240299823633157"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts(normalize=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression <a class=\"anchor\" id=\"lgr\"></a>\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score:  0.9875073486184597\n",
      "Testing Score:  0.8893298059964727\n"
     ]
    }
   ],
   "source": [
    "pipe = make_pipeline(CountVectorizer(stop_words='english'), StandardScaler(with_mean=False), LogisticRegression())\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "print('Training Score: ', pipe.score(X_train, y_train))\n",
    "print('Testing Score: ', pipe.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eh, not great, but also not terrible for a simple model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score:  0.8793356848912405\n",
      "Testing Score:  0.8734567901234568\n"
     ]
    }
   ],
   "source": [
    "pipe = make_pipeline(CountVectorizer(stop_words='english', max_features=100, ngram_range=(1,2)), StandardScaler(with_mean=False), LogisticRegression())\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "print('Training Score: ', pipe.score(X_train, y_train))\n",
    "print('Testing Score: ', pipe.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing ngram range doesn't seem to do much here. We'll try a proper grid search with some other parameters to see if we can improve on this at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score:  0.9454732510288066\n",
      "Testing Score:  0.9029982363315696\n",
      "Best Parameters:  {'countvectorizer__max_df': 0.9, 'countvectorizer__max_features': 500, 'countvectorizer__min_df': 3, 'countvectorizer__ngram_range': (1, 1)}\n"
     ]
    }
   ],
   "source": [
    "pipe = make_pipeline(CountVectorizer(stop_words='english'), StandardScaler(with_mean=False),  LogisticRegression())\n",
    "\n",
    "params = {'countvectorizer__max_features': [100, 500],\n",
    "              'countvectorizer__min_df': [2, 3],\n",
    "              'countvectorizer__max_df': [0.9, 0.95],\n",
    "              'countvectorizer__ngram_range': [(1, 1), (1, 2), (1, 3)]}\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid=params)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print('Training Score: ', grid.score(X_train, y_train))\n",
    "print('Testing Score: ', grid.score(X_test, y_test))\n",
    "print('Best Parameters: ', grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, increasing number of features used improves performance of the model. Adding ngrams didn't seem to help all that much. We'll try this set of parameters with some other models to see if this remains true."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN <a class=\"anchor\" id=\"knn\"></a>\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score:  0.8375955320399765\n",
      "Testing Score:  0.7848324514991182\n"
     ]
    }
   ],
   "source": [
    "knn = make_pipeline(CountVectorizer(stop_words='english', max_features=100), StandardScaler(with_mean=False), KNeighborsClassifier())\n",
    "\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "print('Training Score: ', knn.score(X_train, y_train))\n",
    "print('Testing Score: ', knn.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems to be the worst performing model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Decision Tree <a class=\"anchor\" id=\"dtree\"></a>\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score:  0.9331275720164609\n",
      "Testing Score:  0.859347442680776\n"
     ]
    }
   ],
   "source": [
    "dt = make_pipeline(CountVectorizer(stop_words='english', max_features=100), DecisionTreeClassifier())\n",
    "\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "print('Training Score: ', dt.score(X_train, y_train))\n",
    "print('Testing Score: ', dt.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seems to be some evidence of overfitting (as is expected) we'll try a random forest model as well in another notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes <a class=\"anchor\" id=\"naive-bayes\"></a>\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score:  0.876249265138154\n",
      "Testing Score:  0.8783068783068783\n"
     ]
    }
   ],
   "source": [
    "nb = make_pipeline(CountVectorizer(stop_words='english', max_features=100), MultinomialNB(alpha=0.00001))\n",
    "\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "print('Training Score: ', nb.score(X_train, y_train))\n",
    "print('Testing Score: ', nb.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model performs relatively well and there doesn't look like evidence of overfitting, model performance maybe improved if more features are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

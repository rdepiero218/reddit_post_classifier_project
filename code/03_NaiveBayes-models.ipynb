{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3: Web APIs & NLP\n",
    "## *Naive Bayes Models*\n",
    "\n",
    "Parameter selection for Naive Bayes models.\n",
    "\n",
    "In this notebook:\n",
    "\n",
    "* [Basic Naive Bayes](#basic-nb)\n",
    "* [Naive Bayes with TfidfVectorizer](#tfid-nb)\n",
    "* [Grid search for CountVectorizer](#gs-cv)\n",
    "* [Determine ideal $\\alpha$](#nb-alpha)\n",
    "* [Best model](#best-model)\n",
    "\n",
    "#### Import Libraries & Read in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## standard imports \n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import re\n",
    "## visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "## preprocessing\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.dummy import DummyClassifier\n",
    "## modeling\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression, Lasso, Ridge, ElasticNet\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "## trees\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.ensemble import BaggingClassifier, BaggingRegressor, RandomForestClassifier, ExtraTreesClassifier, RandomForestRegressor, ExtraTreesRegressor, AdaBoostRegressor, AdaBoostClassifier, GradientBoostingRegressor\n",
    "## NLP\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "## analysis\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix, accuracy_score, make_scorer, f1_score, mean_squared_error\n",
    "\n",
    "## options\n",
    "import sklearn\n",
    "pd.options.display.max_rows = 4000\n",
    "pd.options.display.max_columns = 100\n",
    "pd.set_option('max_colwidth', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "### read in data\n",
    "data = pd.read_csv('../data/reddit_posts_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "### select data\n",
    "X = data['selftext']\n",
    "y = data['is_fallout']\n",
    "### TTS\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Naive Bayes <a class=\"anchor\" id=\"basic-nb\"></a>\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score:  0.9616402116402116\n",
      "Testing Score:  0.9470899470899471\n"
     ]
    }
   ],
   "source": [
    "nb = make_pipeline(CountVectorizer(stop_words='english'), MultinomialNB())\n",
    "\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "print('Training Score: ', nb.score(X_train, y_train))\n",
    "print('Testing Score: ', nb.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes with `TfidfVectorizer`<a class=\"anchor\" id=\"tfid-nb\"></a>\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score:  0.9700176366843033\n",
      "Testing Score:  0.9435626102292769\n"
     ]
    }
   ],
   "source": [
    "nb = make_pipeline(TfidfVectorizer(stop_words='english'), MultinomialNB())\n",
    "\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "print('Training Score: ', nb.score(X_train, y_train))\n",
    "print('Testing Score: ', nb.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is not much difference between `TfidfVectorizer` and `countVectorizer` so we'll stick with using `countVectorizer`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid search for CountVectorizer <a class=\"anchor\" id=\"gs-cv\"></a>\n",
    "<hr/>\n",
    "\n",
    "First we'll try and narrow down parameters for CountVectorizer. It was taking too long to do a grid search on everything all together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score:  0.9329805996472663\n",
      "Testing Score:  0.9241622574955908\n",
      "Best Parameters:  {'countvectorizer__max_df': 0.9, 'countvectorizer__max_features': 800, 'countvectorizer__min_df': 2, 'countvectorizer__ngram_range': (1, 1)}\n"
     ]
    }
   ],
   "source": [
    "pipe = make_pipeline(CountVectorizer(stop_words='english'),  MultinomialNB())\n",
    "\n",
    "params = {\n",
    "    'countvectorizer__max_features': [100, 500, 800],\n",
    "    'countvectorizer__min_df': [2, 3],\n",
    "    'countvectorizer__max_df': [0.9, 0.95],\n",
    "    'countvectorizer__ngram_range': [(1, 1), (1, 2)],       \n",
    "}\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid=params)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print('Training Score: ', grid.score(X_train, y_train))\n",
    "print('Testing Score: ', grid.score(X_test, y_test))\n",
    "print('Best Parameters: ', grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each time we try to adjust number of features it seems to choose the max value. We'll just switch to using all features for subsequent models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine ideal $\\alpha$ <a class=\"anchor\" id=\"nb-alpha\"></a>\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score:  0.9607583774250441\n",
      "Testing Score:  0.9475308641975309\n",
      "Best Parameters:  {'countvectorizer__max_df': 0.9, 'countvectorizer__min_df': 2, 'countvectorizer__ngram_range': (1, 1), 'multinomialnb__alpha': 0.01}\n"
     ]
    }
   ],
   "source": [
    "pipe = make_pipeline(CountVectorizer(stop_words='english'),  MultinomialNB())\n",
    "\n",
    "params = {\n",
    "    'countvectorizer__min_df': [2, 3],\n",
    "    'countvectorizer__max_df': [0.9, 0.95],\n",
    "    'countvectorizer__ngram_range': [(1, 1), (1, 2)],\n",
    "    'multinomialnb__alpha': [0.01, 0.0001, 0.0001, 0.00001]         \n",
    "}\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid=params)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print('Training Score: ', grid.score(X_train, y_train))\n",
    "print('Testing Score: ', grid.score(X_test, y_test))\n",
    "print('Best Parameters: ', grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score training:  0.9577999051683261\n",
      "F1 Score testing:  0.9437883797827114\n"
     ]
    }
   ],
   "source": [
    "print('F1 Score training: ', f1_score(y_train, grid.predict(X_train)))\n",
    "print('F1 Score testing: ', f1_score(y_test, grid.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score:  0.9542870677404074\n"
     ]
    }
   ],
   "source": [
    "y_preds = grid.predict(X)\n",
    "print('F1 Score: ', f1_score(y, y_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Model <a class=\"anchor\" id=\"best-model\"></a>\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 Training Score:  0.9610523221634333\n",
      "R2 Testing Score:  0.9444444444444444\n",
      "-----------------------\n",
      "F1 Score training:  0.9581292463264338\n",
      "F1 Score testing:  0.9405099150141643\n",
      "-----------------------\n",
      "Complete F1 Score:  0.9537113768201729\n"
     ]
    }
   ],
   "source": [
    "pipe = make_pipeline(CountVectorizer(stop_words='english'),  MultinomialNB())\n",
    "\n",
    "params = {\n",
    "    'countvectorizer__min_df': [2],\n",
    "    'countvectorizer__max_df': [0.9],\n",
    "    'countvectorizer__ngram_range': [(1, 1)],\n",
    "    'multinomialnb__alpha': [0.0001]         \n",
    "}\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid=params)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print('R2 Training Score: ', grid.score(X_train, y_train))\n",
    "print('R2 Testing Score: ', grid.score(X_test, y_test))\n",
    "print('-----------------------')\n",
    "print('F1 Score training: ', f1_score(y_train, grid.predict(X_train)))\n",
    "print('F1 Score testing: ', f1_score(y_test, grid.predict(X_test)))\n",
    "y_preds = grid.predict(X)\n",
    "print('-----------------------')\n",
    "print('Complete F1 Score: ', f1_score(y, y_preds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
